{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DAQ-card data analysis\n",
    "\n",
    "### This script analyzes the recorded serial output from the daq card.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bisect import bisect_left\n",
    "import itertools\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variables for configuration\n",
    "\n",
    "data_file = \"20171110-1240_Thresh_scan_start-5_stop-200_step-5_sec_per_rec_120_detector_mode_2E.txt\"\n",
    "\n",
    "max_coincidence_time_s = 100*10**(-9)  # 100 ns\n",
    "\n",
    "active_channels = [1,2,3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def simplecount(filename):\n",
    "    lines = 0\n",
    "    for line in open(filename, 'U'):\n",
    "        lines += 1\n",
    "    return lines\n",
    "\n",
    "def check_line_for_corruption(line):\n",
    "    # check for obvious corruption\n",
    "    if len(line.split(' ')) is not 16:\n",
    "        return True\n",
    "    # split line\n",
    "    split_line = line.split(\" \")\n",
    "    # check each field for currupted-ness\n",
    "    for i in range(len(split_line)):\n",
    "        length = len(split_line[i])\n",
    "        if i in (0,9):\n",
    "            if length != 8:\n",
    "                return True\n",
    "        if i in range(1,9):\n",
    "            if length != 2:\n",
    "                return True\n",
    "        if i == 10:\n",
    "            if length != 10:\n",
    "                return True\n",
    "        if i == 11:\n",
    "            if length != 6:\n",
    "                return True\n",
    "        if i == 12:\n",
    "            if length != 1:\n",
    "                return True\n",
    "        if i == 13:\n",
    "            if length != 2:\n",
    "                return True\n",
    "        if i == 14:\n",
    "            if length != 1:\n",
    "                return True\n",
    "        if i == 15:\n",
    "            if length != 6:\n",
    "                return True\n",
    "        \n",
    "    # all checks passed\n",
    "    return False\n",
    "\n",
    "def findClosest(list_to_search, value):\n",
    "    # bisect only works because we know that our list is sorted\n",
    "    pos = bisect_left(list_to_search, value)\n",
    "    if pos == 0:\n",
    "        return list_to_search[0]\n",
    "    if pos == len(list_to_search):\n",
    "        return list_to_search[-1]\n",
    "    before = list_to_search[pos - 1]\n",
    "    after = list_to_search[pos]\n",
    "    if after - value < value - before:\n",
    "       return after\n",
    "    else:\n",
    "       return before\n",
    "\n",
    "def parse_edge_data_forgetfull_parser(file_lines):\n",
    "    edge_and_tirgger_pos = [[] for i in range(9)]\n",
    "\n",
    "    previous_PPS = -1000\n",
    "    current_PPS = -1000\n",
    "\n",
    "    rollover_counter_trig = 0\n",
    "    rollover_counter_pps = 0\n",
    "    old_unadjusted_time_trig = 0\n",
    "    old_unadjusted_time_pps = 0\n",
    "\n",
    "    # list for saving our data\n",
    "    edge_and_tirgger_pos = [[] for i in range(9)]\n",
    "\n",
    "    lines_to_read = len(file_lines)\n",
    "    print(\"Lines to read: \"+ str(lines_to_read))\n",
    "\n",
    "    print(\"Starting to parse section\")\n",
    "    line_count = 0\n",
    "    discarded_lines = 0\n",
    "    line_buffer = []\n",
    "    event_lengths = []\n",
    "    discarded_event_lengths = []\n",
    "\n",
    "    for into_buffer_line in file_lines:\n",
    "        line_count += 1\n",
    "        if (line_count % (lines_to_read//20)) == 0:\n",
    "            print(\"Percentage of section parsed: \" + str(round( float(line_count)*100 / float(lines_to_read), 2)))\n",
    "        # buffer lines to make sure the event is not corrupted\n",
    "        # empty the buffer when a corrupt line was found\n",
    "        if check_line_for_corruption(into_buffer_line):\n",
    "            discarded_lines += 1 + len(line_buffer)\n",
    "            discarded_event_lengths.append(len(line_buffer)+1)\n",
    "            line_buffer = []\n",
    "            continue\n",
    "        # if the buffer was empty add the line and continue\n",
    "        if len(line_buffer) == 0:\n",
    "            line_buffer.append(into_buffer_line)\n",
    "            continue\n",
    "        # if the event has come to an end; parse it, otherwise just buffer the line\n",
    "        new_timestamp = int(into_buffer_line.split(\" \")[0], 16)\n",
    "        old_timestamp = int(line_buffer[0].split(\" \")[0], 16)\n",
    "        if new_timestamp == old_timestamp:\n",
    "            line_buffer.append(into_buffer_line)    \n",
    "        else:\n",
    "            event_lengths.append(len(line_buffer))\n",
    "            for current_line in line_buffer:\n",
    "                # split line\n",
    "                split_line = current_line.split(\" \")        \n",
    "\n",
    "                # parse the line\n",
    "                try:\n",
    "                    # get the current pps and trigger\n",
    "                    pps = int(split_line[9], 16)\n",
    "                    trigger_clock = int(split_line[0], 16)\n",
    "                    # check for rollovers\n",
    "                    if old_unadjusted_time_pps > pps:\n",
    "                        rollover_counter_pps += 1\n",
    "                    if old_unadjusted_time_trig > trigger_clock:\n",
    "                        rollover_counter_trig += 1\n",
    "                    # adjust for rollovers\n",
    "                    old_unadjusted_time_pps = pps\n",
    "                    pps = rollover_counter_pps * 2**36 + pps\n",
    "                    old_unadjusted_time_trig = trigger_clock\n",
    "                    trigger_clock = rollover_counter_trig * 2**36 + trigger_clock\n",
    "\n",
    "                    # save pps\n",
    "                    if pps != current_PPS:\n",
    "                        previous_PPS = current_PPS\n",
    "                        current_PPS = pps\n",
    "                    # check if we already have a previous pps\n",
    "                    if previous_PPS < 0:\n",
    "                        continue\n",
    "\n",
    "                    # calculate the UTC time\n",
    "                    day = int(split_line[11][0:2])\n",
    "                    month = int(split_line[11][2:4])\n",
    "                    year = int(split_line[11][4:6])\n",
    "                    hour = int(split_line[10][0:2])\n",
    "                    minute = int(split_line[10][2:4])\n",
    "                    second = float(split_line[10][4:10])\n",
    "                    second = int(round(second + float(split_line[15])/1000 ))\n",
    "                    time_from_gps = datetime.datetime(year+2000,\n",
    "                                                    month,\n",
    "                                                    day,\n",
    "                                                    hour,\n",
    "                                                    minute,\n",
    "                                                    second,\n",
    "                                                    tzinfo=None)\n",
    "                    unix_seconds = (time_from_gps - datetime.datetime(1970,1,1,tzinfo=None)).total_seconds()\n",
    "                    clock_rate = current_PPS - previous_PPS\n",
    "                    sub_sec = float(trigger_clock - current_PPS) / float(clock_rate)    \n",
    "\n",
    "                    # convert stuff into the binary representation\n",
    "                    bin_rep = []\n",
    "                    for i in range(1,9):\n",
    "                        bin_rep.append(bin(int(split_line[i],16))[2:].zfill(8))\n",
    "\n",
    "                    # check if the daq triggered\n",
    "                    triggered = False\n",
    "                    if bin_rep[0][0] == '1':\n",
    "                        triggered =  True\n",
    "                    if triggered:\n",
    "                        edge_and_tirgger_pos[0].append(unix_seconds+sub_sec)\n",
    "\n",
    "                    # check rising and falling edges\n",
    "                    for i in range(len(bin_rep)):\n",
    "                        bin_number = bin_rep[i]\n",
    "                        if bin_number[2] == '1':\n",
    "                            edge_nano_secs = float(int(bin_number[3:],2))/32 * 24\n",
    "                            edge_sub_secs = edge_nano_secs * 10**(-9)\n",
    "                            edge_and_tirgger_pos[i+1].append(unix_seconds+sub_sec+edge_sub_secs)\n",
    "                except ValueError:\n",
    "                    print(\"Caught a value error, that had slipped through the corruption check. Line: \"+ str(line_count))\n",
    "                    print(\"Errorous line: \"+ current_line)\n",
    "                    discarded_lines += 1\n",
    "                    break\n",
    "            # empty the buffer and pre-save the next line\n",
    "            line_buffer = []\n",
    "            line_buffer.append(into_buffer_line)  \n",
    "\n",
    "    print(\"Percentage of lines that were corrputed: \"+str(100.0*discarded_lines/float(line_count)))\n",
    "    print(\"Average event length: {:.2f}\".format(np.mean(event_lengths)))\n",
    "    print(\"Median event length: {:.0f}\".format(np.median(event_lengths)))\n",
    "    print(\"Stdabw. event length: {:.2f}\".format(np.std(event_lengths)))\n",
    "    print(\"Average discarded event length: {:.2f}\".format(np.mean(discarded_event_lengths)))\n",
    "    print(\"Median discarded event length: {:.0f}\".format(np.median(discarded_event_lengths)))\n",
    "    print(\"Stdabw. discarded event length: {:.2f}\".format(np.std(discarded_event_lengths)))\n",
    "    return edge_and_tirgger_pos\n",
    "\n",
    "def calculate_correlations_from_edge_data(edge_data, max_coincidence_time_s, active_channels=[1,2,3]):\n",
    "    # calculate correlations\n",
    "\n",
    "    combinations = list(itertools.combinations(detector_area.keys(), 2))\n",
    "    correlations_to_do = []\n",
    "    for i in range(len(detector_area.keys())):\n",
    "        correlations_to_do.append((combinations[i], detector_area.keys()[2-i]))\n",
    "\n",
    "    correlations = {}\n",
    "    for corr_tupel in correlations_to_do:\n",
    "        (key1, key2), key3 = corr_tupel\n",
    "        # do the first correlation\n",
    "        base_correlations = {'base_time':[], 'diff':[]}\n",
    "\n",
    "        for key1_time in edge_data[key1*2+1]:\n",
    "            closest_key2_time = findClosest(edge_data[key2*2+1], key1_time)\n",
    "            diff = closest_key2_time - key1_time\n",
    "            if (abs(diff) <= max_coincidence_time_s):\n",
    "                base_correlations['base_time'].append(key1_time)\n",
    "                base_correlations['diff'].append(diff)\n",
    "        correlations[\"CH\"+str(key1)+\"<->CH\"+str(key2)] = base_correlations\n",
    "\n",
    "        second_level_correlation = {'base_time':[], 'diff':[]}\n",
    "        # do second correlation\n",
    "        for base_time in base_correlations['base_time']:\n",
    "            closest_key3_time = findClosest(edge_data[key3*2+1], base_time)\n",
    "            diff = closest_key3_time - base_time\n",
    "            if (abs(diff) <= max_coincidence_time_s):\n",
    "                second_level_correlation['base_time'].append(base_time)\n",
    "                second_level_correlation['diff'].append(diff)\n",
    "\n",
    "        correlations[\"[CH\"+str(key1)+\"<->CH\"+str(key2)+\"]<->CH\"+str(key3)] = second_level_correlation\n",
    "    \n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '20171110-1240_Thresh_scan_start-5_stop-200_step-5_sec_per_rec_120_detector_mode_2E.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-733afe443f6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Counting lines\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlines_to_read\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimplecount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Lines to read: \"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines_to_read\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Starting to parse the file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-6abef3bb69ed>\u001b[0m in \u001b[0;36msimplecount\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msimplecount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'U'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: '20171110-1240_Thresh_scan_start-5_stop-200_step-5_sec_per_rec_120_detector_mode_2E.txt'"
     ]
    }
   ],
   "source": [
    "print(\"Counting lines\")\n",
    "lines_to_read = simplecount(data_file)\n",
    "print(\"Lines to read: \"+ str(lines_to_read))\n",
    "\n",
    "print(\"Starting to parse the file\")\n",
    "line_count = 0\n",
    "recording_secs_per_setting = 0\n",
    "current_threshold = 0\n",
    "\n",
    "threshold_list = []\n",
    "count_lists = []\n",
    "edge_data_list = []\n",
    "edge_data_lines = []\n",
    "\n",
    "for current_line in open(data_file, 'U'):\n",
    "    line_count += 1\n",
    "    if (line_count % (lines_to_read//20)) == 0:\n",
    "        print(\"Percentage of file read: \" + str(round( float(line_count)*100 / float(lines_to_read), 2)))\n",
    "    \n",
    "    if \"recording_secs_per_setting:\" in current_line:\n",
    "        split_line = current_line.replace(\"\\n\",\"\").split(\":\")\n",
    "        recording_secs_per_setting = float(split_line[len(split_line)-1])\n",
    "        continue\n",
    "    \n",
    "    if \"Current Threshold:\" in current_line:\n",
    "        split_line = current_line.replace(\"\\n\",\"\").split(\":\")\n",
    "        current_threshold = int(split_line[len(split_line)-1])\n",
    "        if len(edge_data_lines) != 0:\n",
    "            edge_data_list.append(parse_edge_data_forgetfull_parser(edge_data_lines))\n",
    "            edge_data_lines = []\n",
    "        continue\n",
    "    \n",
    "    if \"DS S0\" in current_line:\n",
    "        split_line = current_line.split(\" \")\n",
    "        data = [int(split_line[i+1].split(\"=\")[1].replace(\"\\n\",\"\"),16) for i in range(5)]\n",
    "        threshold_list.append(current_threshold)\n",
    "        count_lists.append(data)\n",
    "        continue\n",
    "    \n",
    "    edge_data_lines.append(current_line)\n",
    "    \n",
    "\n",
    "count_lists = np.asarray(count_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'edge_data_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f5b61685dfbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# do correlations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcorrs_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[1;32mfor\u001b[0m \u001b[0medge_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0medge_data_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcorrs_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcalculate_correlations_from_edge_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medge_and_tirgger_pos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_coincidence_time_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactive_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'edge_data_list' is not defined"
     ]
    }
   ],
   "source": [
    "# do correlations\n",
    "corrs_list = []\n",
    "efficency_percentages = []\n",
    "\n",
    "\n",
    "for edge_data in edge_data_list:\n",
    "    # calculate the correlations per section\n",
    "    corrs = calculate_correlations_from_edge_data(edge_and_tirgger_pos, max_coincidence_time_s, active_channels=active_channels)\n",
    "    corrs_list.append(corrs)\n",
    "    # caclate the efficency percentages\n",
    "    percentages_list = [0 for in range(len(active_channels))]\n",
    "    for key in corrs.keys():\n",
    "        if key[0] == '[':\n",
    "            percentage = 100.0*len(corrs[key]['diff'])/len(corrs[key[1:10]]['diff'])\n",
    "            percentages_dict[int(key[16])-1] = percentage\n",
    "\n",
    "efficency_percentages = np.asarray(efficency_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create folder for the plot dump\n",
    "directory = \"plot_save/\"+data_file.split('.')[0]+\"/\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# create plots\n",
    "for i in range(len(count_lists[0])):\n",
    "    if count_lists[:,i].any() == 0:\n",
    "        continue\n",
    "    if i == 4:\n",
    "        plt.plot(threshold_list, count_lists[:,i], label=\"Threefold trigger\")\n",
    "    else:\n",
    "        plt.plot(threshold_list, count_lists[:,i], label=\"CH\"+str(i))\n",
    "    plt.legend()\n",
    "    title = \"Threshold scan - CH\"+str(i)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Threshold [mV]\")\n",
    "    plt.ylabel(\"Counts [1]\")\n",
    "    plt.savefig(directory+title+\".png\")\n",
    "    plt.show()\n",
    "\n",
    "for i in range(len(count_lists[0])):\n",
    "    if count_lists[:,i].any() == 0:\n",
    "        continue\n",
    "    if i == 4:\n",
    "        plt.semilogy(threshold_list, count_lists[:,i], label=\"Threefold trigger\")\n",
    "    else:\n",
    "        plt.semilogy(threshold_list, count_lists[:,i], label=\"CH\"+str(i))\n",
    "plt.legend()\n",
    "title = \"All Channels - logarithmic y-axis\"\n",
    "plt.title(title)\n",
    "plt.xlabel(\"Threshold [mV]\")\n",
    "plt.ylabel(\"Counts [1]\")\n",
    "plt.savefig(directory+title+\".png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

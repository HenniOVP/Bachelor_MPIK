{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DAQ-card data analysis\n",
    "\n",
    "### This script analyzes the recorded serial output from the daq card.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bisect import bisect_left\n",
    "import itertools\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variables for configuration\n",
    "\n",
    "data_file = \"20171120-1305_DAQ_1800_sec_threshs_300-289-292-288_detector_mode_1E_det1_slab1.txt\"\n",
    "#data_file = \"20171109-1219_DAQ_3000_sec_threshs_70-14-79-83_detector_mode_1E.txt\"\n",
    "#data_file = \"20171110-1019_DAQ_3000_sec_threshs_70-14-70-83_detector_mode_1E.txt\"\n",
    "#data_file = \"20171110-1118_DAQ_1800_sec_threshs_70-14-60-83_detector_mode_1E.txt\"\n",
    "#data_file = \"20171110-1140_DAQ_1800_sec_threshs_70-14-50-83_detector_mode_1E.txt\"\n",
    "\n",
    "# the V1 pulse is 2.5 ms long\n",
    "# the one from the muon hunter is 1 ms long\n",
    "max_coincidence_time_s = 100*10**(-9)  # 100 ns\n",
    "\n",
    "\n",
    "# detector areas in cm^2\n",
    "detector_area = {\n",
    "                1: 400.0,\n",
    "                2: 400.0,\n",
    "                3: 400.0,\n",
    "                }\n",
    "max_Hz = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def simplecount(filename):\n",
    "    lines = 0\n",
    "    for line in open(filename, 'U'):\n",
    "        lines += 1\n",
    "    return lines\n",
    "\n",
    "def check_line_for_corruption(line):\n",
    "    # check for obvious corruption\n",
    "    if len(line.split(' ')) is not 16:\n",
    "        return True\n",
    "    # split line\n",
    "    split_line = line.split(\" \")\n",
    "    # check each field for currupted-ness\n",
    "    for i in range(len(split_line)):\n",
    "        length = len(split_line[i])\n",
    "        if i in (0,9):\n",
    "            if length != 8:\n",
    "                return True\n",
    "        if i in range(1,9):\n",
    "            if length != 2:\n",
    "                return True\n",
    "        if i == 10:\n",
    "            if length != 10:\n",
    "                return True\n",
    "        if i == 11:\n",
    "            if length != 6:\n",
    "                return True\n",
    "        if i == 12:\n",
    "            if length != 1:\n",
    "                return True\n",
    "        if i == 13:\n",
    "            if length != 2:\n",
    "                return True\n",
    "        if i == 14:\n",
    "            if length != 1:\n",
    "                return True\n",
    "        if i == 15:\n",
    "            if length != 6:\n",
    "                return True\n",
    "        \n",
    "    # all checks passed\n",
    "    return False\n",
    "\n",
    "def findClosest(list_to_search, value):\n",
    "    # bisect only works because we know that our list is sorted\n",
    "    pos = bisect_left(list_to_search, value)\n",
    "    if pos == 0:\n",
    "        return list_to_search[0]\n",
    "    if pos == len(list_to_search):\n",
    "        return list_to_search[-1]\n",
    "    before = list_to_search[pos - 1]\n",
    "    after = list_to_search[pos]\n",
    "    if after - value < value - before:\n",
    "       return after\n",
    "    else:\n",
    "       return before\n",
    "\n",
    "def parse_edge_data_forgetfull_parser(file_lines):\n",
    "    edge_and_tirgger_pos = [[] for i in range(9)]\n",
    "\n",
    "    previous_PPS = -1000\n",
    "    current_PPS = -1000\n",
    "\n",
    "    rollover_counter_trig = 0\n",
    "    rollover_counter_pps = 0\n",
    "    old_unadjusted_time_trig = 0\n",
    "    old_unadjusted_time_pps = 0\n",
    "\n",
    "    # list for saving our data\n",
    "    edge_and_tirgger_pos = [[] for i in range(9)]\n",
    "\n",
    "    lines_to_read = len(file_lines)\n",
    "    print(\"Lines to read: \"+ str(lines_to_read))\n",
    "\n",
    "    print(\"Starting to parse section\")\n",
    "    line_count = 0\n",
    "    discarded_lines = 0\n",
    "    line_buffer = []\n",
    "    event_lengths = []\n",
    "    discarded_event_lengths = []\n",
    "\n",
    "    for into_buffer_line in file_lines:\n",
    "        line_count += 1\n",
    "        if (line_count % (lines_to_read//20)) == 0:\n",
    "            print(\"Percentage of section parsed: \" + str(round( float(line_count)*100 / float(lines_to_read), 2)))\n",
    "        # buffer lines to make sure the event is not corrupted\n",
    "        # empty the buffer when a corrupt line was found\n",
    "        if check_line_for_corruption(into_buffer_line):\n",
    "            discarded_lines += 1 + len(line_buffer)\n",
    "            discarded_event_lengths.append(len(line_buffer)+1)\n",
    "            line_buffer = []\n",
    "            continue\n",
    "        # if the buffer was empty add the line and continue\n",
    "        if len(line_buffer) == 0:\n",
    "            line_buffer.append(into_buffer_line)\n",
    "            continue\n",
    "        # if the event has come to an end; parse it, otherwise just buffer the line\n",
    "        new_timestamp = int(into_buffer_line.split(\" \")[0], 16)\n",
    "        old_timestamp = int(line_buffer[0].split(\" \")[0], 16)\n",
    "        if new_timestamp == old_timestamp:\n",
    "            line_buffer.append(into_buffer_line)    \n",
    "        else:\n",
    "            event_lengths.append(len(line_buffer))\n",
    "            for current_line in line_buffer:\n",
    "                # split line\n",
    "                split_line = current_line.split(\" \")        \n",
    "\n",
    "                # parse the line\n",
    "                try:\n",
    "                    # get the current pps and trigger\n",
    "                    pps = int(split_line[9], 16)\n",
    "                    trigger_clock = int(split_line[0], 16)\n",
    "                    # check for rollovers\n",
    "                    if old_unadjusted_time_pps > pps:\n",
    "                        rollover_counter_pps += 1\n",
    "                    if old_unadjusted_time_trig > trigger_clock:\n",
    "                        rollover_counter_trig += 1\n",
    "                    # adjust for rollovers\n",
    "                    old_unadjusted_time_pps = pps\n",
    "                    pps = rollover_counter_pps * 2**36 + pps\n",
    "                    old_unadjusted_time_trig = trigger_clock\n",
    "                    trigger_clock = rollover_counter_trig * 2**36 + trigger_clock\n",
    "\n",
    "                    # save pps\n",
    "                    if pps != current_PPS:\n",
    "                        previous_PPS = current_PPS\n",
    "                        current_PPS = pps\n",
    "                    # check if we already have a previous pps\n",
    "                    if previous_PPS < 0:\n",
    "                        continue\n",
    "\n",
    "                    # calculate the UTC time\n",
    "                    day = int(split_line[11][0:2])\n",
    "                    month = int(split_line[11][2:4])\n",
    "                    year = int(split_line[11][4:6])\n",
    "                    hour = int(split_line[10][0:2])\n",
    "                    minute = int(split_line[10][2:4])\n",
    "                    second = float(split_line[10][4:10])\n",
    "                    second = int(round(second + float(split_line[15])/1000 ))\n",
    "                    time_from_gps = datetime.datetime(year+2000,\n",
    "                                                    month,\n",
    "                                                    day,\n",
    "                                                    hour,\n",
    "                                                    minute,\n",
    "                                                    second,\n",
    "                                                    tzinfo=None)\n",
    "                    unix_seconds = (time_from_gps - datetime.datetime(1970,1,1,tzinfo=None)).total_seconds()\n",
    "                    clock_rate = current_PPS - previous_PPS\n",
    "                    sub_sec = float(trigger_clock - current_PPS) / float(clock_rate)    \n",
    "\n",
    "                    # convert stuff into the binary representation\n",
    "                    bin_rep = []\n",
    "                    for i in range(1,9):\n",
    "                        bin_rep.append(bin(int(split_line[i],16))[2:].zfill(8))\n",
    "\n",
    "                    # check if the daq triggered\n",
    "                    triggered = False\n",
    "                    if bin_rep[0][0] == '1':\n",
    "                        triggered =  True\n",
    "                    if triggered:\n",
    "                        edge_and_tirgger_pos[0].append(unix_seconds+sub_sec)\n",
    "\n",
    "                    # check rising and falling edges\n",
    "                    for i in range(len(bin_rep)):\n",
    "                        bin_number = bin_rep[i]\n",
    "                        if bin_number[2] == '1':\n",
    "                            edge_nano_secs = float(int(bin_number[3:],2))/32 * 24\n",
    "                            edge_sub_secs = edge_nano_secs * 10**(-9)\n",
    "                            edge_and_tirgger_pos[i+1].append(unix_seconds+sub_sec+edge_sub_secs)\n",
    "                except ValueError:\n",
    "                    print(\"Caught a value error, that had slipped through the corruption check. Line: \"+ str(line_count))\n",
    "                    print(\"Errorous line: \"+ current_line)\n",
    "                    discarded_lines += 1\n",
    "                    break\n",
    "            # empty the buffer and pre-save the next line\n",
    "            line_buffer = []\n",
    "            line_buffer.append(into_buffer_line)  \n",
    "\n",
    "    print(\"Percentage of lines that were corrputed: \"+str(100.0*discarded_lines/float(line_count)))\n",
    "    print(\"Average event length: {:.2f}\".format(np.mean(event_lengths)))\n",
    "    print(\"Median event length: {:.0f}\".format(np.median(event_lengths)))\n",
    "    print(\"Stdabw. event length: {:.2f}\".format(np.std(event_lengths)))\n",
    "    print(\"Average discarded event length: {:.2f}\".format(np.mean(discarded_event_lengths)))\n",
    "    print(\"Median discarded event length: {:.0f}\".format(np.median(discarded_event_lengths)))\n",
    "    print(\"Stdabw. discarded event length: {:.2f}\".format(np.std(discarded_event_lengths)))\n",
    "    return edge_and_tirgger_pos\n",
    "\n",
    "def calculate_correlations_from_edge_data(edge_data, max_coincidence_time_s, active_channels=[1,2,3]):\n",
    "    # calculate correlations\n",
    "\n",
    "    combinations = list(itertools.combinations(detector_area.keys(), 2))\n",
    "    correlations_to_do = []\n",
    "    for i in range(len(detector_area.keys())):\n",
    "        correlations_to_do.append((combinations[i], detector_area.keys()[2-i]))\n",
    "\n",
    "    correlations = {}\n",
    "    for corr_tupel in correlations_to_do:\n",
    "        (key1, key2), key3 = corr_tupel\n",
    "        # do the first correlation\n",
    "        base_correlations = {'base_time':[], 'diff':[]}\n",
    "\n",
    "        for key1_time in edge_data[key1*2+1]:\n",
    "            closest_key2_time = findClosest(edge_data[key2*2+1], key1_time)\n",
    "            diff = closest_key2_time - key1_time\n",
    "            if (abs(diff) <= max_coincidence_time_s):\n",
    "                base_correlations['base_time'].append(key1_time)\n",
    "                base_correlations['diff'].append(diff)\n",
    "        correlations[\"CH\"+str(key1)+\"<->CH\"+str(key2)] = base_correlations\n",
    "\n",
    "        second_level_correlation = {'base_time':[], 'diff':[]}\n",
    "        # do second correlation\n",
    "        for base_time in base_correlations['base_time']:\n",
    "            closest_key3_time = findClosest(edge_data[key3*2+1], base_time)\n",
    "            diff = closest_key3_time - base_time\n",
    "            if (abs(diff) <= max_coincidence_time_s):\n",
    "                second_level_correlation['base_time'].append(base_time)\n",
    "                second_level_correlation['diff'].append(diff)\n",
    "\n",
    "        correlations[\"[CH\"+str(key1)+\"<->CH\"+str(key2)+\"]<->CH\"+str(key3)] = second_level_correlation\n",
    "    \n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines to read: 26799\n",
      "Starting to parse section\n",
      "Percentage of section parsed: 5.0\n",
      "Percentage of section parsed: 9.99\n",
      "Percentage of section parsed: 14.99\n",
      "Percentage of section parsed: 19.99\n",
      "Percentage of section parsed: 24.98\n",
      "Percentage of section parsed: 29.98\n",
      "Percentage of section parsed: 34.98\n",
      "Percentage of section parsed: 39.97\n",
      "Percentage of section parsed: 44.97\n",
      "Percentage of section parsed: 49.96\n",
      "Percentage of section parsed: 54.96\n",
      "Percentage of section parsed: 59.96\n",
      "Percentage of section parsed: 64.95\n",
      "Percentage of section parsed: 69.95\n",
      "Percentage of section parsed: 74.95\n",
      "Percentage of section parsed: 79.94\n",
      "Percentage of section parsed: 84.94\n",
      "Percentage of section parsed: 89.94\n",
      "Percentage of section parsed: 94.93\n",
      "Percentage of section parsed: 99.93\n",
      "Percentage of lines that were corrputed: 0.00746296503601\n",
      "Average event length: 1.50\n",
      "Median event length: 1\n",
      "Stdabw. event length: 0.66\n",
      "Average discarded event length: 1.00\n",
      "Median discarded event length: 1\n",
      "Stdabw. discarded event length: 0.00\n"
     ]
    }
   ],
   "source": [
    "lines_to_parse = []\n",
    "for line in open(data_file, 'U'):\n",
    "    lines_to_parse.append(line)\n",
    "\n",
    "edge_and_tirgger_pos = parse_edge_data_forgetfull_parser(lines_to_parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 9234)\n",
      "(1, 0)\n",
      "(2, 0)\n",
      "(3, 8608)\n",
      "(4, 6992)\n",
      "(5, 8171)\n",
      "(6, 5222)\n",
      "(7, 8099)\n",
      "(8, 5067)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(edge_and_tirgger_pos)):\n",
    "    print(i, len(edge_and_tirgger_pos[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CH1<->CH2: 7327\n",
      "CH1<->CH3: 7295\n",
      "CH2<->CH3: 6860\n",
      "[CH1<->CH2]<->CH3: 6167\n",
      "Percentage of CH1<->CH2: 84.17\n",
      "[CH1<->CH3]<->CH2: 6167\n",
      "Percentage of CH1<->CH3: 84.54\n",
      "[CH2<->CH3]<->CH1: 6157\n",
      "Percentage of CH2<->CH3: 89.75\n"
     ]
    }
   ],
   "source": [
    "corrs = calculate_correlations_from_edge_data(edge_and_tirgger_pos, max_coincidence_time_s, active_channels=[1,2,3])\n",
    "\n",
    "keys = sorted(corrs.keys())\n",
    "for key in keys:\n",
    "    print(key+\": {:d}\".format(len(corrs[key]['diff'])))\n",
    "    if key[0] == '[':\n",
    "        print(\"Percentage of \"+key[1:10]+\": {:.2f}\".format(100.0*len(corrs[key]['diff'])/len(corrs[key[1:10]]['diff'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1798.394207239151, 29.973236787319184, 1511181344.7552507, 1511179546.3610435)\n"
     ]
    }
   ],
   "source": [
    "# calculate test runntime\n",
    "min_s_time = edge_and_tirgger_pos[0][0]\n",
    "max_s_time = edge_and_tirgger_pos[0][len(edge_and_tirgger_pos[0])-1]\n",
    "s_time = (max_s_time - min_s_time)\n",
    "print(s_time, s_time/60, max_s_time, min_s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum coincidence time [s]: 1e-07\n",
      "Test runtime [min]: 19.9758048654\n",
      "\n",
      "CH1 statistics:\n",
      "\tDetector area [cm^2]: 400.0\n",
      "\tTotal number of events: 5197\n",
      "\tEfficency [events/cm^2/min]: 0.650411840102\n",
      "\n",
      "CH2 statistics:\n",
      "\tDetector area [cm^2]: 400.0\n",
      "\tTotal number of events: 6339\n",
      "\tEfficency [events/cm^2/min]: 0.793334742045\n",
      "\n",
      "CH3 statistics:\n",
      "\tDetector area [cm^2]: 400.0\n",
      "\tTotal number of events: 5578\n",
      "\tEfficency [events/cm^2/min]: 0.698094524551\n",
      "\n",
      "Coincidence [CH2<->CH3]<->CH1\n",
      "\tTotal number of coincidences: 4302\n",
      "\tEfficency [events/cm^2/min]: 0.538401334639\n",
      "\tTime between events for one coincidence:\n",
      "\t\tMedian [s]: 0\n",
      "\t\tAverage [s]: 0\n",
      "\t\tStandard deviation [s]: 0\n",
      "\n",
      "Coincidence [CH1<->CH3]<->CH2\n",
      "\tTotal number of coincidences: 4290\n",
      "\tEfficency [events/cm^2/min]: 0.536899517806\n",
      "\tTime between events for one coincidence:\n",
      "\t\tMedian [s]: 0\n",
      "\t\tAverage [s]: 0\n",
      "\t\tStandard deviation [s]: 0\n",
      "\n",
      "Coincidence [CH1<->CH2]<->CH3\n",
      "\tTotal number of coincidences: 4290\n",
      "\tEfficency [events/cm^2/min]: 0.536899517806\n",
      "\tTime between events for one coincidence:\n",
      "\t\tMedian [s]: 0\n",
      "\t\tAverage [s]: 0\n",
      "\t\tStandard deviation [s]: 0\n",
      "\n",
      "Coincidence CH2<->CH3\n",
      "\tTotal number of coincidences: 5474\n",
      "\tEfficency [events/cm^2/min]: 0.685078778664\n",
      "\tTime between events for one coincidence:\n",
      "\t\tMedian [s]: 0\n",
      "\t\tAverage [s]: 0\n",
      "\t\tStandard deviation [s]: 0\n",
      "\n",
      "Coincidence CH1<->CH3\n",
      "\tTotal number of coincidences: 4355\n",
      "\tEfficency [events/cm^2/min]: 0.545034358985\n",
      "\tTime between events for one coincidence:\n",
      "\t\tMedian [s]: 0\n",
      "\t\tAverage [s]: 0\n",
      "\t\tStandard deviation [s]: 0\n",
      "\n",
      "Coincidence CH1<->CH2\n",
      "\tTotal number of coincidences: 5066\n",
      "\tEfficency [events/cm^2/min]: 0.634017006342\n",
      "\tTime between events for one coincidence:\n",
      "\t\tMedian [s]: 0\n",
      "\t\tAverage [s]: 0\n",
      "\t\tStandard deviation [s]: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tell statistics\n",
    "print(\"Maximum coincidence time [s]: {:.4g}\".format(max_coincidence_time_s))\n",
    "print(\"Test runtime [min]: \" + str(s_time/60))\n",
    "print(\"\")\n",
    "\n",
    "for key in detector_area.keys():\n",
    "    print(\"CH\"+str(key)+\" statistics:\")\n",
    "    print(\"\\tDetector area [cm^2]: \" + str(detector_area[key]))\n",
    "    print(\"\\tTotal number of events: \" + str(len(edge_and_tirgger_pos[key*2+1])))\n",
    "    print(\"\\tEfficency [events/cm^2/min]: \" + str(len(edge_and_tirgger_pos[key*2+1])/detector_area[key]/(s_time/60)))\n",
    "    print(\"\")\n",
    "\n",
    "keys = reversed(sorted(corrs.keys()))\n",
    "for key in keys:\n",
    "    print(\"Coincidence \"+key)\n",
    "\n",
    "    print(\"\\tTotal number of coincidences: \" +str(len(corrs[key]['diff'])))\n",
    "    print(\"\\tEfficency [events/cm^2/min]: \"+str(len(corrs[key]['diff'])/detector_area[1]/(s_time/60)))\n",
    "    \n",
    "    print(\"\\tTime between events for one coincidence:\")\n",
    "    print(\"\\t\\tMedian [s]: {:.4g}\".format(np.median(corrs[key]['diff'])))\n",
    "    print(\"\\t\\tAverage [s]: {:.4g}\".format(np.mean(corrs[key]['diff'])))\n",
    "    print(\"\\t\\tStandard deviation [s]: {:.4g}\".format(np.std(corrs[key]['diff'])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
